# üöÄ D√©ploiement d'un Cluster Kubernetes 1.34 avec KubeAdm

> **Environnement** : 1 Master (controlplane) + 1 Worker (node1) sur Ubuntu 22.04  
> **Outils** : Vagrant + VirtualBox, kubeadm, containerd, Calico CNI, Helm, Kubernetes Dashboard  
> **Version Kubernetes** : 1.34.x

---

## üìã Table des Mati√®res

1. [Architecture et pr√©requis](#1-architecture-et-pr√©requis)
2. [Provisionnement des VMs avec Vagrant](#2-provisionnement-des-vms-avec-vagrant)
3. [Pr√©paration de tous les n≈ìuds (controlplane + worker)](#3-pr√©paration-de-tous-les-n≈ìuds-controlplane--worker)
4. [Installation des composants Kubernetes](#4-installation-des-composants-kubernetes)
5. [Initialisation du cluster (Master uniquement)](#5-initialisation-du-cluster-master-uniquement)
6. [Jonction du Worker au cluster](#6-jonction-du-worker-au-cluster)
7. [D√©ploiement du plugin CNI : Calico](#7-d√©ploiement-du-plugin-cni--calico)
8. [Test du cluster : Application Guestbook](#8-test-du-cluster--application-guestbook)
9. [Installation de Helm](#9-installation-de-helm)
10. [D√©ploiement du Kubernetes Dashboard](#10-d√©ploiement-du-kubernetes-dashboard)
11. [Cr√©ation des utilisateurs Dashboard (read-only et admin)](#11-cr√©ation-des-utilisateurs-dashboard-read-only-et-admin)
12. [Acc√®s au Dashboard via port-forward](#12-acc√®s-au-dashboard-via-port-forward)

---

## 1. Architecture et pr√©requis

### Topologie du cluster

| R√¥le          | Hostname      | IP             | CPU | RAM  |
|---------------|---------------|----------------|-----|------|
| Control Plane | controlplane  | 192.168.90.10  | 2   | 4 Go |
| Worker Node   | node1         | 192.168.90.11  | 2   | 3 Go |

### R√©seaux Kubernetes

| R√©seau        | CIDR            | Usage                                    |
|---------------|-----------------|------------------------------------------|
| N≈ìuds (VMs)  | 192.168.90.0/24 | R√©seau h√¥te Vagrant (existant)           |
| Pods (Calico) | 10.244.0.0/16   | IPs attribu√©es aux pods                  |
| Services      | 10.96.0.0/12    | IPs virtuelles des services (d√©faut K8s) |

> ‚ö†Ô∏è **Pourquoi `10.244.0.0/16` pour les pods et pas `192.168.0.0/16` ?**  
> Le r√©seau des VMs est `192.168.90.0/24`. Or `192.168.0.0/16` englobe **toute** la plage `192.168.x.x`,
> ce qui inclut tes n≈ìuds. Kubernetes routerait alors le trafic vers tes VMs via le r√©seau pod
> au lieu de l'interface physique ‚Üí pannes r√©seau impr√©visibles.  
> `10.244.0.0/16` est enti√®rement distinct : aucun chevauchement possible.

### Pr√©requis sur la machine h√¥te

- [VirtualBox](https://www.virtualbox.org/) ‚â• 6.1 install√©
- [Vagrant](https://www.vagrantup.com/) ‚â• 2.3 install√©
- Au moins 8 Go de RAM disponible sur l'h√¥te
- Connexion Internet (pour le pull des images)

> üìñ **Doc officielle** : https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#before-you-begin

---

## 2. Provisionnement des VMs avec Vagrant

### Vagrantfile

```ruby
# -*- mode: ruby -*-
# vi: set ft=ruby :
Vagrant.configure("2") do |config|
    config.vm.define "controlplane" do |controlplane|
      controlplane.vm.box = "eazytraining/ubuntu2204"
      controlplane.vm.box_version = "1.0"
      controlplane.vm.network "private_network", type: "static", ip: "192.168.90.10"
      controlplane.vm.hostname = "controlplane"
      controlplane.vm.provider "virtualbox" do |v|
        v.name = "controlplane"
        v.memory = 4096
        v.cpus = 2
      end
    end
    nodes=1
    ram_node=3072
    cpu_node=2
    (1..nodes).each do |i|
      config.vm.define "node#{i}" do |node|
        node.vm.box = "eazytraining/ubuntu2204"
        node.vm.box_version = "1.0"
        node.vm.network "private_network", type: "static", ip: "192.168.90.1#{i}"
        node.vm.hostname = "node#{i}"
        node.vm.provider "virtualbox" do |v|
          v.name = "node#{i}"
          v.memory = ram_node
          v.cpus = cpu_node
        end
      end
    end
  end
```

### D√©marrage des VMs

```powershell
# Dans le dossier contenant le Vagrantfile
vagrant up

# V√©rification que les VMs sont actives
vagrant status
```

### Connexion aux VMs

```powershell
# Se connecter au master
vagrant ssh controlplane

# Se connecter au worker (depuis un autre terminal)
vagrant ssh node1
```

---

## 3. Pr√©paration de tous les n≈ìuds (controlplane + worker)

> ‚ö†Ô∏è **Les commandes de cette section doivent √™tre ex√©cut√©es sur TOUS les n≈ìuds** (controlplane ET node1).

> üìñ **Doc officielle** : https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#installing-kubeadm-kubelet-and-kubectl

### 3.1 Mise √† jour du syst√®me

```bash
sudo apt-get update && sudo apt-get upgrade -y
```

### 3.2 Configuration du fichier /etc/hosts

Ajouter les entr√©es sur chaque n≈ìud pour la r√©solution DNS locale :

```bash
sudo tee -a /etc/hosts <<EOF
192.168.90.10 controlplane
192.168.90.11 node1
EOF
```

### 3.3 D√©sactivation du swap

> üìñ **Doc officielle** : https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#before-you-begin
>
> Kubernetes exige que le swap soit d√©sactiv√©. Sans cette √©tape, `kubeadm init` √©chouera.

```bash
# D√©sactivation imm√©diate
sudo swapoff -a

# D√©sactivation permanente (commenter la ligne swap dans fstab)
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab

# V√©rification : la colonne Swap doit afficher 0B
free -h
```

### 3.4 Activation des modules kernel n√©cessaires

> üìñ **Doc officielle** : https://kubernetes.io/docs/setup/production-environment/container-runtimes/#forwarding-ipv4-and-letting-iptables-see-bridged-traffic

```bash
# Chargement des modules au d√©marrage
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF

# Activation imm√©diate des modules
sudo modprobe overlay
sudo modprobe br_netfilter

# Configuration des param√®tres sysctl (networking)
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
EOF

# Application des param√®tres sans red√©marrage
sudo sysctl --system

# V√©rification
lsmod | grep br_netfilter
lsmod | grep overlay
```

### 3.5 Installation du container runtime : containerd

> üìñ **Doc officielle** : https://kubernetes.io/docs/setup/production-environment/container-runtimes/#containerd

```bash
export DEBIAN_FRONTEND=noninteractive
# Installation des d√©pendances
sudo apt-get install -y apt-transport-https ca-certificates curl gnupg lsb-release

# Ajout du repository Docker (contient containerd)
sudo mkdir -p /etc/apt/keyrings
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | \
  sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg

echo "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] \
  https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable" | \
  sudo tee /etc/apt/sources.list.d/docker.list > /dev/null

sudo apt-get update

# Installation de containerd
sudo apt-get install -y containerd.io

# G√©n√©ration de la configuration par d√©faut
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml > /dev/null

# IMPORTANT : Activer SystemdCgroup = true (requis par Kubernetes avec systemd)
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/g' /etc/containerd/config.toml

# V√©rification de la modification
grep 'SystemdCgroup' /etc/containerd/config.toml
# Doit afficher : SystemdCgroup = true

# Red√©marrage et activation de containerd
sudo systemctl restart containerd
sudo systemctl enable containerd

# V√©rification du statut
sudo systemctl status containerd
```

---

## 4. Installation des composants Kubernetes

> ‚ö†Ô∏è **Les commandes de cette section doivent √™tre ex√©cut√©es sur TOUS les n≈ìuds** (controlplane ET node1).

> üìñ **Doc officielle** : https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#installing-kubeadm-kubelet-and-kubectl

```bash
# Installation des d√©pendances
sudo apt-get install -y apt-transport-https ca-certificates curl gpg

# Cr√©ation du dossier keyrings s'il n'existe pas
sudo mkdir -p -m 755 /etc/apt/keyrings

# Ajout de la cl√© GPG du repository Kubernetes 1.34
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.34/deb/Release.key | \
  sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg

# Ajout du repository Kubernetes 1.34
echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] \
  https://pkgs.k8s.io/core:/stable:/v1.34/deb/ /' | \
  sudo tee /etc/apt/sources.list.d/kubernetes.list

# Mise √† jour et installation des trois composants
sudo apt-get update
sudo apt-get install -y kubelet kubeadm kubectl

# Verrouillage des versions pour √©viter les mises √† jour automatiques non souhait√©es
sudo apt-mark hold kubelet kubeadm kubectl

# V√©rification des versions install√©es
kubeadm version
kubectl version --client
kubelet --version
```

---

## 5. Initialisation du cluster (Master uniquement)

> ‚ö†Ô∏è **Les commandes de cette section doivent √™tre ex√©cut√©es UNIQUEMENT sur le controlplane.**

> üìñ **Doc officielle** : https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#initializing-your-control-plane-node

### 5.1 Pr√©-pull des images (optionnel mais recommand√©)

```bash
# V√©rifier quelles images seront t√©l√©charg√©es
sudo kubeadm config images list --kubernetes-version=v1.34.0

# Pr√©-t√©l√©chargement des images (acc√©l√®re l'init et d√©tecte les erreurs r√©seau t√¥t)
sudo kubeadm config images pull --kubernetes-version=v1.34.0
```

### 5.2 Initialisation du cluster

```bash
sudo kubeadm init \
  --apiserver-advertise-address=192.168.90.10 \
  --pod-network-cidr=10.244.0.0/16 \
  --kubernetes-version=v1.34.0 \
  --node-name=controlplane \
  --cri-socket=unix:///var/run/containerd/containerd.sock
```

> üí° **Important** : √Ä la fin de l'init, kubeadm affiche la commande `kubeadm join`.
> **Copiez et conservez cette commande** ‚Äî elle sera utilis√©e √† l'√©tape 6.
>
> Elle ressemble √† ceci :
> ```
> kubeadm join 192.168.90.10:6443 --token <TOKEN> \
>   --discovery-token-ca-cert-hash sha256:<HASH>
> ```

### 5.3 Configuration de kubectl pour l'utilisateur courant

```bash
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

# V√©rification : le master appara√Æt en "NotReady" (normal, le CNI n'est pas encore install√©)
kubectl get nodes
kubectl get pods -n kube-system
```

---

## 6. Jonction du Worker au cluster

> ‚ö†Ô∏è **Les commandes de cette section doivent √™tre ex√©cut√©es UNIQUEMENT sur node1.**

> üìñ **Doc officielle** : https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#join-nodes

```bash
# Ex√©cuter la commande kubeadm join copi√©e lors de l'√©tape 5.2
# Exemple (vos valeurs de token et hash seront diff√©rentes) :
sudo kubeadm join 192.168.90.10:6443 \
  --token <VOTRE_TOKEN> \
  --discovery-token-ca-cert-hash sha256:<VOTRE_HASH> \
  --node-name=node1 \
  --cri-socket=unix:///var/run/containerd/containerd.sock
```

### Si le token a expir√© (valable 24h par d√©faut)

```bash
# G√©n√©rer un nouveau token depuis le controlplane, avec la commande join compl√®te
sudo kubeadm token create --print-join-command
```

### V√©rification depuis le controlplane

```bash
# Les n≈ìuds sont en "NotReady" jusqu'√† l'installation du CNI ‚Äî c'est normal
kubectl get nodes
```

---

## 7. D√©ploiement du plugin CNI : Calico

> ‚ö†Ô∏è **Les commandes de cette section doivent √™tre ex√©cut√©es UNIQUEMENT sur le controlplane.**

> üìñ **Doc officielle Kubernetes (addons)** : https://kubernetes.io/docs/concepts/cluster-administration/addons/#networking-and-network-policy  
> üìñ **Doc officielle Calico** : https://docs.tigera.io/calico/latest/getting-started/kubernetes/self-managed-onprem/onpremises

### Pourquoi Calico ?

Calico est la solution CNI la plus utilis√©e en production. Elle offre des network policies avanc√©es (pare-feu entre pods), de bonnes performances et une compatibilit√© totale avec kubeadm.

### Installation via l'op√©rateur Tigera

```bash
# √âtape 1 : Installer l'op√©rateur Tigera et ses CRDs
kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.29.0/manifests/tigera-operator.yaml

# V√©rification de l'op√©rateur
kubectl get pods -n tigera-operator

# √âtape 2 : T√©l√©charger le fichier de configuration pour l'adapter
curl -O https://raw.githubusercontent.com/projectcalico/calico/v3.29.0/manifests/custom-resources.yaml
```

### Adaptation du CIDR

Le fichier `custom-resources.yaml` contient par d√©faut `192.168.0.0/16`.
Il faut le remplacer par `10.244.0.0/16` pour correspondre exactement √† ce qui a √©t√© pass√© √† `kubeadm init` :

```bash
# Remplacer le CIDR Calico par d√©faut par notre CIDR
sed -i 's|192.168.0.0/16|10.244.0.0/16|g' custom-resources.yaml

# V√©rification
grep 'cidr' custom-resources.yaml
# Doit afficher : cidr: 10.244.0.0/16

# Appliquer la configuration
kubectl create -f custom-resources.yaml

# Suivre le d√©marrage des pods Calico (attendre que tous soient en Running)
watch kubectl get pods -n calico-system
```

### V√©rification finale du cluster

```bash
# Tous les n≈ìuds doivent d√©sormais √™tre en √©tat "Ready"
kubectl get nodes -o wide

# Tous les pods syst√®me doivent √™tre en Running
kubectl get pods -n kube-system
kubectl get pods -n calico-system
```

---

## 8. Test du cluster : Application Guestbook

> üìñ **Doc officielle** : https://kubernetes.io/docs/tutorials/stateless-application/guestbook/

L'application Guestbook est l'exemple officiel Kubernetes. Elle illustre une architecture multi-tier :
un backend Redis (1 leader + 2 followers) et un frontend PHP (3 replicas) expos√© via NodePort.

### 8.1 D√©ploiement du backend Redis Leader

```bash
cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-leader
  labels:
    app: redis
    role: leader
    tier: backend
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
  template:
    metadata:
      labels:
        app: redis
        role: leader
        tier: backend
    spec:
      containers:
      - name: leader
        image: docker.io/redis:6.0.5
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379
EOF

cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Service
metadata:
  name: redis-leader
  labels:
    app: redis
    role: leader
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: leader
    tier: backend
EOF
```

### 8.2 D√©ploiement des Redis Followers

```bash
cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-follower
  labels:
    app: redis
    role: follower
    tier: backend
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
  template:
    metadata:
      labels:
        app: redis
        role: follower
        tier: backend
    spec:
      containers:
      - name: follower
        image: gcr.io/google_samples/gb-redis-follower:v2
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379
EOF

cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Service
metadata:
  name: redis-follower
  labels:
    app: redis
    role: follower
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: follower
    tier: backend
EOF
```

### 8.3 D√©ploiement du Frontend PHP

```bash
cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google_samples/gb-frontend:v5
        env:
        - name: GET_HOSTS_FROM
          value: dns
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80
EOF

# Service NodePort pour acc√©der √† l'app depuis la machine h√¥te
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  type: NodePort
  ports:
  - port: 80
    nodePort: 30007
  selector:
    app: guestbook
    tier: frontend
EOF
```

### 8.4 V√©rification du d√©ploiement

```bash
# V√©rifier que tous les pods sont Running et bien distribu√©s sur les n≈ìuds
kubectl get pods -o wide

# V√©rifier les services
kubectl get services

# Acc√©der √† l'application depuis la machine h√¥te
# http://192.168.90.10:30007  (via le master)
# http://192.168.90.11:30007  (via le worker)
```

### 8.5 Test du scaling

```bash
# Scale le frontend de 3 √† 5 replicas
kubectl scale deployment frontend --replicas=5

# Observer la distribution des nouveaux pods sur les deux n≈ìuds
kubectl get pods -o wide | grep frontend

# Revenir √† 3 replicas
kubectl scale deployment frontend --replicas=3
```

### 8.6 Test de la communication r√©seau inter-pods

```bash
# Lancer un pod de debug temporaire pour tester la r√©solution DNS et la connectivit√©
kubectl run test-net --image=busybox --rm -it --restart=Never -- sh

# Dans le pod (depuis le r√©seau pod 10.244.x.x) :
nslookup redis-leader      # r√©solution DNS du service Redis Leader
nslookup frontend          # r√©solution DNS du service frontend
wget -q -O- frontend       # requ√™te HTTP vers le frontend PHP
exit
```

### 8.7 Nettoyage de l'application de test

```bash
kubectl delete deployment frontend redis-leader redis-follower
kubectl delete service frontend redis-leader redis-follower
```

---

## 9. Installation de Helm

> üìñ **Doc officielle Helm** : https://helm.sh/docs/intro/install/

Helm est le gestionnaire de packages pour Kubernetes. Il simplifie l'installation d'applications complexes comme le Dashboard Kubernetes.

```bash
# Sur le controlplane ‚Äî installation via le script officiel
curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
chmod 700 get_helm.sh
./get_helm.sh

# V√©rification
helm version

# Autocompl√©tion bash (optionnel mais pratique)
echo 'source <(helm completion bash)' >> ~/.bashrc
source ~/.bashrc
```

---

## 10. D√©ploiement du Kubernetes Dashboard

> üìñ **Doc officielle** : https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/  
> üìñ **ArtifactHub chart** : https://artifacthub.io/packages/helm/k8s-dashboard/kubernetes-dashboard

### 10.1 Installation via Helm

```bash
# Ajout du repository Helm pour le Dashboard
helm repo add kubernetes-dashboard https://kubernetes.github.io/dashboard/
helm repo update

# Installation dans son namespace d√©di√©
helm upgrade --install kubernetes-dashboard kubernetes-dashboard/kubernetes-dashboard \
  --create-namespace \
  --namespace kubernetes-dashboard

# Attendre que tous les pods soient en Running
kubectl get pods -n kubernetes-dashboard --watch

# V√©rifier les services cr√©√©s ‚Äî noter le nom exact du service proxy
kubectl get svc -n kubernetes-dashboard
```

> üí° Le chart v7+ embarque Kong comme proxy par d√©faut. Le service exposant le Dashboard
> s'appelle g√©n√©ralement `kubernetes-dashboard-kong-proxy` sur le port 443.
> V√©rifier avec `kubectl get svc -n kubernetes-dashboard` avant l'√©tape 12.

---

## 11. Cr√©ation des utilisateurs Dashboard (read-only et admin)

> üìñ **Doc officielle RBAC** : https://kubernetes.io/docs/reference/access-authn-authz/rbac/  
> üìñ **Cr√©ation d'un sample user** : https://github.com/kubernetes/dashboard/blob/master/docs/user/access-control/creating-sample-user.md

### 11.1 Utilisateur Admin (lecture + √©criture compl√®te)

```bash
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: ServiceAccount
metadata:
  name: dashboard-admin
  namespace: kubernetes-dashboard
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: dashboard-admin
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: dashboard-admin
  namespace: kubernetes-dashboard
---
# Secret long-lived : g√©n√®re un token permanent associ√© au ServiceAccount
apiVersion: v1
kind: Secret
metadata:
  name: dashboard-admin-secret
  namespace: kubernetes-dashboard
  annotations:
    kubernetes.io/service-account.name: "dashboard-admin"
type: kubernetes.io/service-account-token
EOF
```

### 11.2 Utilisateur Read-Only (visualisation uniquement)

```bash
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: ServiceAccount
metadata:
  name: dashboard-readonly
  namespace: kubernetes-dashboard
---
# ClusterRole personnalis√© : uniquement get/list/watch sur les ressources essentielles
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: dashboard-viewonly
rules:
- apiGroups: [""]
  resources:
  - configmaps
  - endpoints
  - persistentvolumeclaims
  - pods
  - pods/log
  - pods/status
  - replicationcontrollers
  - serviceaccounts
  - services
  - nodes
  - persistentvolumes
  - namespaces
  - events
  - limitranges
  - resourcequotas
  verbs: ["get", "list", "watch"]
- apiGroups: ["apps"]
  resources:
  - daemonsets
  - deployments
  - replicasets
  - statefulsets
  verbs: ["get", "list", "watch"]
- apiGroups: ["autoscaling"]
  resources:
  - horizontalpodautoscalers
  verbs: ["get", "list", "watch"]
- apiGroups: ["batch"]
  resources:
  - cronjobs
  - jobs
  verbs: ["get", "list", "watch"]
- apiGroups: ["networking.k8s.io"]
  resources:
  - ingresses
  - networkpolicies
  verbs: ["get", "list", "watch"]
- apiGroups: ["storage.k8s.io"]
  resources:
  - storageclasses
  verbs: ["get", "list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: dashboard-readonly
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: dashboard-viewonly
subjects:
- kind: ServiceAccount
  name: dashboard-readonly
  namespace: kubernetes-dashboard
---
apiVersion: v1
kind: Secret
metadata:
  name: dashboard-readonly-secret
  namespace: kubernetes-dashboard
  annotations:
    kubernetes.io/service-account.name: "dashboard-readonly"
type: kubernetes.io/service-account-token
EOF
```

---

## 12. Acc√®s au Dashboard via port-forward

> üìñ **Doc officielle port-forward** : https://kubernetes.io/docs/tasks/access-application-cluster/port-forward-access-application-cluster/

Le `kubectl port-forward` cr√©e un tunnel direct entre la machine qui ex√©cute kubectl et le service cible dans le cluster. C'est la m√©thode la plus simple pour un lab local : aucun LoadBalancer, aucun Ingress Controller requis.

### 12.1 R√©cup√©ration des tokens

```bash
# Token Admin (acc√®s complet read-write)
echo "=== TOKEN ADMIN ==="
kubectl get secret dashboard-admin-secret \
  -n kubernetes-dashboard \
  -o jsonpath="{.data.token}" | base64 --decode
echo -e "\n"

# Token Read-Only (acc√®s en lecture seule)
echo "=== TOKEN READ-ONLY ==="
kubectl get secret dashboard-readonly-secret \
  -n kubernetes-dashboard \
  -o jsonpath="{.data.token}" | base64 --decode
echo -e "\n"
```

### 12.2 Lancer le port-forward

```bash
# V√©rifier d'abord le nom exact du service (peut varier selon la version du chart)
kubectl get svc -n kubernetes-dashboard

# Lancer le port-forward vers le service proxy du Dashboard
# --address=0.0.0.0 permet l'acc√®s depuis la machine h√¥te quand kubectl
# est ex√©cut√© depuis l'int√©rieur d'une VM
kubectl port-forward svc/kubernetes-dashboard-kong-proxy \
  -n kubernetes-dashboard \
  8443:443 \
  --address=0.0.0.0
```

> üí° Laisser ce terminal ouvert pendant toute la session Dashboard.
> Le tunnel se ferme d√®s que la commande est interrompue (Ctrl+C).

### 12.3 Acc√©der au Dashboard depuis le navigateur

Depuis la **machine h√¥te** (port-forward lanc√© dans la VM controlplane) :
```
https://192.168.90.10:8443
```

Si kubectl est install√© **directement sur la machine h√¥te** :
```
https://localhost:8443
```

Ensuite :
1. Accepter l'avertissement de certificat auto-sign√© ("Avanc√©" ‚Üí "Continuer")
2. S√©lectionner **"Token"** comme m√©thode d'authentification
3. Coller le token souhait√© (admin ou read-only) et cliquer **"Sign In"**

### 12.4 Comparaison des deux profils utilisateurs

| Action | Admin | Read-Only |
|--------|-------|-----------|
| Voir pods, deployments, services | ‚úÖ | ‚úÖ |
| Voir les logs des pods | ‚úÖ | ‚úÖ |
| Voir les namespaces et events | ‚úÖ | ‚úÖ |
| Cr√©er / modifier des ressources | ‚úÖ | ‚ùå |
| Supprimer des ressources | ‚úÖ | ‚ùå |
| Scaler des deployments | ‚úÖ | ‚ùå |
| G√©rer les secrets | ‚úÖ | ‚ùå |

### 12.5 Tokens temporaires (alternative pour les d√©mos)

```bash
# Token admin valable 24h (expire automatiquement, plus s√©curis√©)
kubectl create token dashboard-admin \
  -n kubernetes-dashboard --duration=24h

# Token read-only valable 1h
kubectl create token dashboard-readonly \
  -n kubernetes-dashboard --duration=1h
```

---

## üîß Commandes utiles

```bash
# Statut g√©n√©ral du cluster
kubectl get nodes -o wide
kubectl get pods --all-namespaces
kubectl cluster-info

# V√©rifier les √©v√©nements (debug)
kubectl get events --sort-by=.metadata.creationTimestamp

# Logs d'un pod
kubectl logs <pod-name> -n <namespace>
kubectl logs <pod-name> -n <namespace> --previous   # logs du crash pr√©c√©dent

# D√©crire un n≈ìud (ressources, events, pods planifi√©s)
kubectl describe node controlplane
kubectl describe node node1

# Red√©marrer un d√©ploiement proprement
kubectl rollout restart deployment/<nom>

# Afficher la configuration kubeadm du cluster
kubectl -n kube-system get configmap kubeadm-config -o yaml
```

---

## ‚ö†Ô∏è Troubleshooting

### Un n≈ìud reste en "NotReady"

```bash
# V√©rifier les logs du kubelet
sudo journalctl -u kubelet -f --no-pager | tail -50

# V√©rifier les pods CNI Calico
kubectl get pods -n calico-system
kubectl describe pod <pod-calico> -n calico-system
```

### Erreur lors du kubeadm init

```bash
# R√©initialiser compl√®tement et recommencer
sudo kubeadm reset
sudo rm -rf /etc/cni/net.d
sudo iptables -F && sudo iptables -t nat -F
sudo ipvsadm --clear 2>/dev/null || true
```

### Token kubeadm join expir√©

```bash
# G√©n√©rer un nouveau token avec la commande join compl√®te depuis le controlplane
sudo kubeadm token create --print-join-command
```

### Le port-forward se coupe

```bash
# Le port-forward n'est pas persistant ‚Äî relancer simplement la commande.
# Pour le garder actif en arri√®re-plan :
kubectl port-forward svc/kubernetes-dashboard-kong-proxy \
  -n kubernetes-dashboard 8443:443 --address=0.0.0.0 &

# Pour l'arr√™ter :
kill %1
```

### Calico pods en "Init" ou "CrashLoopBackOff"

```bash
# V√©rifier que le CIDR dans la configuration Calico correspond √† kubeadm init
kubectl get installation default -o jsonpath='{.spec.calicoNetwork.ipPools[0].cidr}'
# Doit afficher : 10.244.0.0/16

# V√©rifier les logs du pod calico-node
kubectl logs -n calico-system \
  $(kubectl get pod -n calico-system -l k8s-app=calico-node -o name | head -1)
```

---

## üìö R√©f√©rences

| Sujet | Lien |
|-------|------|
| Pr√©requis kubeadm | https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/ |
| Cr√©er un cluster kubeadm | https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/ |
| Container runtimes (containerd) | https://kubernetes.io/docs/setup/production-environment/container-runtimes/ |
| Calico CNI | https://docs.tigera.io/calico/latest/getting-started/kubernetes/self-managed-onprem/onpremises |
| App Guestbook (officielle) | https://kubernetes.io/docs/tutorials/stateless-application/guestbook/ |
| RBAC | https://kubernetes.io/docs/reference/access-authn-authz/rbac/ |
| port-forward | https://kubernetes.io/docs/tasks/access-application-cluster/port-forward-access-application-cluster/ |
| Helm installation | https://helm.sh/docs/intro/install/ |
| Kubernetes Dashboard | https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/ |
| Dashboard sample user | https://github.com/kubernetes/dashboard/blob/master/docs/user/access-control/creating-sample-user.md |

---

*R√©dig√© pour Kubernetes v1.34 sur Ubuntu 22.04 ‚Äî F√©vrier 2026*
